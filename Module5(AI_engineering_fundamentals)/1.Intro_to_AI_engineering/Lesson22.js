// Safety Best Practices for AI Engineering

/*
Before deploying AI models, especially those that generate content, it's crucial to implement safety best practices to mitigate risks associated with harmful or biased outputs. Here are some key strategies:
1. Content Filtering:
- Implement content filtering mechanisms to detect and block harmful, offensive, or inappropriate content generated by AI models.
- Use pre-trained classifiers or develop custom filters tailored to your application's needs.
2. Bias Mitigation:
- Regularly evaluate AI models for biases in their outputs.
- Use diverse and representative training datasets to minimize bias.
- Implement techniques such as re-weighting or adversarial training to reduce bias in model predictions.
3. User Feedback:
- Incorporate user feedback mechanisms to report inappropriate or harmful content.
- Use this feedback to continuously improve content filtering and model performance.
4. Rate Limiting:
- Implement rate limiting to prevent abuse of AI services, such as spamming or generating excessive content.
5. Transparency:
- Clearly communicate to users when they are interacting with AI-generated content.
- Provide disclaimers about the potential limitations and risks associated with AI-generated content.
6. Regular Audits:
- Conduct regular audits of AI models and their outputs to ensure compliance with safety standards.
- Update models and safety protocols based on audit findings.
7. Human-in-the-Loop:
- For high-stakes applications, consider implementing a human-in-the-loop approach where human reviewers can oversee and approve AI-generated content before it is published or used.
By following these safety best practices, AI engineers can help ensure that their applications are responsible, ethical, and safe for users.
*/

import OpenAI from "openai";

const openai = new OpenAI({
    dangerouslyAllowBrowser: true
});

async function main() {
  // Relevant for both inputs and outputs
  const completion = await openai.moderations.create({
    input: "I hate you!",
    user: 'user_123432423'
  });
  const {flagged, categories} = completion.results[0];
  // console.log("flagged", flagged);
  // console.log("categories", categories);

  if (flagged) {
      renderWarning(categories);
  }
}

main();

function renderWarning(obj) {
  const keys = Object.keys(obj);
  const filtered = keys.filter((key) => obj[key]);
  document.body.innerText =
    `Your response has been flagged for the following reasons: ${filtered.join(", ")}.`
}

// In this way, we can ensure that any harmful or inappropriate content is detected and handled appropriately before being presented to users.